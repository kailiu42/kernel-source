Patch-mainline: Never, in PREEMPT_RT repository
References: PREEMPT_RT
Acked-by: Kai Liu <kai.liu@suse.com>
Subject: mm: memcontrol: Add an argument to refill_stock() to indicate locking
From: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date: Thu May 20 12:33:07 2021 +0200

From: Sebastian Andrzej Siewior <bigeasy@linutronix.de>

The access to the per-CPU variable memcg_stock is protected by disabling
interrupts. refill_stock() may change the ->caching member and updates
the ->nr_pages member.
refill_obj_stock() is also accecssing memcg_stock (modifies ->nr_pages)
and disables interrupts as part for the locking. Since
refill_obj_stock() may invoke refill_stock() (via drain_obj_stock() ->
obj_cgroup_uncharge_pages()) the "disable interrupts"-lock is acquired
recursively.

Add an argument to refill_stock() to indicate if it is required to
disable interrupts as part of the locking for exclusive memcg_stock
access.

Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Signed-off-by: Thomas Gleixner <tglx@linutronix.de>


---
 mm/memcontrol.c | 27 ++++++++++++++++-----------
 1 file changed, 16 insertions(+), 11 deletions(-)
---
diff --git a/mm/memcontrol.c b/mm/memcontrol.c
index 238707ecbf3e..39b2e3d09d87 100644
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@ -256,7 +256,8 @@ struct cgroup_subsys_state *vmpressure_to_css(struct vmpressure *vmpr)
 extern spinlock_t css_set_lock;
 
 static void obj_cgroup_uncharge_pages(struct obj_cgroup *objcg,
-				      unsigned int nr_pages);
+				      unsigned int nr_pages,
+				      bool lock_memcg_stock);
 
 static void obj_cgroup_release(struct percpu_ref *ref)
 {
@@ -293,7 +294,7 @@ static void obj_cgroup_release(struct percpu_ref *ref)
 	spin_lock_irqsave(&css_set_lock, flags);
 	memcg = obj_cgroup_memcg(objcg);
 	if (nr_pages)
-		obj_cgroup_uncharge_pages(objcg, nr_pages);
+		obj_cgroup_uncharge_pages(objcg, nr_pages, false);
 	list_del(&objcg->list);
 	mem_cgroup_put(memcg);
 	spin_unlock_irqrestore(&css_set_lock, flags);
@@ -2151,12 +2152,14 @@ static void drain_local_stock(struct work_struct *dummy)
  * Cache charges(val) to local per_cpu area.
  * This will be consumed by consume_stock() function, later.
  */
-static void refill_stock(struct mem_cgroup *memcg, unsigned int nr_pages)
+static void refill_stock(struct mem_cgroup *memcg, unsigned int nr_pages,
+			 bool lock_memcg_stock)
 {
 	struct memcg_stock_pcp *stock;
 	unsigned long flags;
 
-	local_irq_save(flags);
+	if (lock_memcg_stock)
+		local_irq_save(flags);
 
 	stock = this_cpu_ptr(&memcg_stock);
 	if (stock->cached != memcg) { /* reset if necessary */
@@ -2169,7 +2172,8 @@ static void refill_stock(struct mem_cgroup *memcg, unsigned int nr_pages)
 	if (stock->nr_pages > MEMCG_CHARGE_BATCH)
 		drain_stock(stock);
 
-	local_irq_restore(flags);
+	if (lock_memcg_stock)
+		local_irq_restore(flags);
 }
 
 /*
@@ -2653,7 +2657,7 @@ static int try_charge(struct mem_cgroup *memcg, gfp_t gfp_mask,
 
 done_restock:
 	if (batch > nr_pages)
-		refill_stock(memcg, batch - nr_pages);
+		refill_stock(memcg, batch - nr_pages, true);
 
 	/*
 	 * If the hierarchy is above the normal consumption range, schedule
@@ -2897,7 +2901,8 @@ static void memcg_free_cache_id(int id)
  * @nr_pages: number of pages to uncharge
  */
 static void obj_cgroup_uncharge_pages(struct obj_cgroup *objcg,
-				      unsigned int nr_pages)
+				      unsigned int nr_pages,
+				      bool lock_memcg_stock)
 {
 	struct mem_cgroup *memcg;
 
@@ -2905,7 +2910,7 @@ static void obj_cgroup_uncharge_pages(struct obj_cgroup *objcg,
 
 	if (!cgroup_subsys_on_dfl(memory_cgrp_subsys))
 		page_counter_uncharge(&memcg->kmem, nr_pages);
-	refill_stock(memcg, nr_pages);
+	refill_stock(memcg, nr_pages, lock_memcg_stock);
 
 	css_put(&memcg->css);
 }
@@ -2992,7 +2997,7 @@ void __memcg_kmem_uncharge_page(struct page *page, int order)
 		return;
 
 	objcg = __page_objcg(page);
-	obj_cgroup_uncharge_pages(objcg, nr_pages);
+	obj_cgroup_uncharge_pages(objcg, nr_pages, true);
 	page->memcg_data = 0;
 	obj_cgroup_put(objcg);
 }
@@ -3028,7 +3033,7 @@ static void drain_obj_stock(struct memcg_stock_pcp *stock)
 		unsigned int nr_bytes = stock->nr_bytes & (PAGE_SIZE - 1);
 
 		if (nr_pages)
-			obj_cgroup_uncharge_pages(old, nr_pages);
+			obj_cgroup_uncharge_pages(old, nr_pages, false);
 
 		/*
 		 * The leftover is flushed to the centralized per-memcg value.
@@ -6904,7 +6909,7 @@ void mem_cgroup_uncharge_skmem(struct mem_cgroup *memcg, unsigned int nr_pages)
 
 	mod_memcg_state(memcg, MEMCG_SOCK, -nr_pages);
 
-	refill_stock(memcg, nr_pages);
+	refill_stock(memcg, nr_pages, true);
 }
 
 static int __init cgroup_memory(char *s)
