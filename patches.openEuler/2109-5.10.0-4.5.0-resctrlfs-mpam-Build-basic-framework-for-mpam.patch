Git-commit: 07285a9dfcaafae0dd3ddac68f7749ce144f5351
From: Yang Yingliang <yangyingliang@huawei.com>
Date: Fri, 26 Feb 2021 20:20:51 +0800
Subject: [PATCH] resctrlfs: mpam: Build basic framework for mpam
Patch-mainline: Queued in openEuler repo, version 5.10.0-4.5.0
References: openEuler-21.03
Git-repo: https://gitee.com/openeuler/kernel.git

hulk inclusion
category: feature
feature: ARM MPAM support
bugzilla: 48265
CVE: NA

--------------------------------

Build basic framework for mpam.

Signed-off-by: Xie XiuQi <xiexiuqi@huawei.com>
Reviewed-by: Hanjun Guo <guohanjun@huawei.com>
Signed-off-by: Yang Yingliang <yangyingliang@huawei.com>
Signed-off-by: Wang ShaoBo <bobo.shaobowang@huawei.com>
Reviewed-by: Cheng Jian <cj.chengjian@huawei.com>
Signed-off-by: Zheng Zengkai <zhengzengkai@huawei.com>
Signed-off-by: Kai Liu <kai.liu@suse.com>
---
 arch/arm64/Kconfig                     |    8 +
 arch/arm64/include/asm/mpam.h          |  335 ++++++++
 arch/arm64/include/asm/mpam_resource.h |   84 ++
 arch/arm64/include/asm/mpam_sched.h    |   48 ++
 arch/arm64/include/asm/resctrl.h       |   66 ++
 arch/arm64/kernel/Makefile             |    1 +
 arch/arm64/kernel/mpam.c               | 1088 ++++++++++++++++++++++++
 arch/arm64/kernel/mpam_ctrlmon.c       |  527 ++++++++++++
 arch/arm64/kernel/mpam_mon.c           |   81 ++
 arch/arm64/kernel/mpam_resource.c      |   14 +
 arch/arm64/kernel/process.c            |    3 +
 fs/Kconfig                             |    9 +
 fs/resctrlfs.c                         |   31 +-
 include/linux/sched.h                  |    2 +-
 14 files changed, 2283 insertions(+), 14 deletions(-)
 create mode 100644 arch/arm64/include/asm/mpam.h
 create mode 100644 arch/arm64/include/asm/mpam_resource.h
 create mode 100644 arch/arm64/include/asm/mpam_sched.h
 create mode 100644 arch/arm64/include/asm/resctrl.h
 create mode 100644 arch/arm64/kernel/mpam.c
 create mode 100644 arch/arm64/kernel/mpam_ctrlmon.c
 create mode 100644 arch/arm64/kernel/mpam_mon.c
 create mode 100644 arch/arm64/kernel/mpam_resource.c

diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
index ce0e131b3c85..06711a03a90d 100644
--- a/arch/arm64/Kconfig
+++ b/arch/arm64/Kconfig
@@ -1004,6 +1004,14 @@ config HOTPLUG_CPU
 	  Say Y here to experiment with turning CPUs off and on.  CPUs
 	  can be controlled through /sys/devices/system/cpu.
 
+config MPAM
+	bool "Support Memory Partitioning and Monitoring"
+	default n
+	select RESCTRL
+	help
+	  Memory Partitioning and Monitoring. More exactly Memory system
+	  performance resource Partitioning and Monitoring
+
 # Common NUMA Features
 config NUMA
 	bool "NUMA Memory Allocation and Scheduler Support"
diff --git a/arch/arm64/include/asm/mpam.h b/arch/arm64/include/asm/mpam.h
new file mode 100644
index 000000000000..4bb583b6a053
--- /dev/null
+++ b/arch/arm64/include/asm/mpam.h
@@ -0,0 +1,335 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _ASM_ARM64_MPAM_H
+#define _ASM_ARM64_MPAM_H
+
+#include <linux/sched.h>
+#include <linux/kernfs.h>
+#include <linux/jump_label.h>
+
+#include <linux/seq_buf.h>
+#include <linux/seq_file.h>
+
+/* MPAM register */
+#define SYS_MPAM0_EL1			sys_reg(3, 0, 10, 5, 1)
+#define SYS_MPAM1_EL1			sys_reg(3, 0, 10, 5, 0)
+#define SYS_MPAM2_EL2			sys_reg(3, 4, 10, 5, 0)
+#define SYS_MPAM3_EL3			sys_reg(3, 6, 10, 5, 0)
+#define SYS_MPAM1_EL12			sys_reg(3, 5, 10, 5, 0)
+#define SYS_MPAMHCR_EL2			sys_reg(3, 4, 10, 4, 0)
+#define SYS_MPAMVPMV_EL2		sys_reg(3, 4, 10, 4, 1)
+#define SYS_MPAMVPMn_EL2(n)		sys_reg(3, 4, 10, 6, n)
+#define SYS_MPAMIDR_EL1			sys_reg(3, 0, 10, 4, 4)
+
+#define MPAM_MASK(n)			((1UL << n) - 1)
+/* plan to use GENMASK(n, 0) instead */
+
+/*
+ * MPAMx_ELn:
+ * 15:0		PARTID_I
+ * 31:16	PARTID_D
+ * 39:32	PMG_I
+ * 47:40	PMG_D
+ * 48		TRAPMPAM1EL1
+ * 49		TRAPMPAM0EL1
+ * 61:49	Reserved
+ * 62		TRAPLOWER
+ * 63		MPAMEN
+ */
+#define PARTID_BITS			(16)
+#define PMG_BITS			(8)
+#define PARTID_MASK			MPAM_MASK(PARTID_BITS)
+#define PMG_MASK			MPAM_MASK(PMG_BITS)
+
+#define PARTID_I_SHIFT			(0)
+#define PARTID_D_SHIFT			(PARTID_I_SHIFT + PARTID_BITS)
+#define PMG_I_SHIFT			(PARTID_D_SHIFT + PARTID_BITS)
+#define PMG_D_SHIFT			(PMG_I_SHIFT + PMG_BITS)
+
+#define PARTID_I_MASK			(PARTID_MASK << PARTID_I_SHIFT)
+#define PARTID_D_MASK			(PARTID_MASK << PARTID_D_SHIFT)
+#define PARTID_I_CLR(r)			((r) & ~PARTID_I_MASK)
+#define PARTID_D_CLR(r)			((r) & ~PARTID_D_MASK)
+#define PARTID_CLR(r)			(PARTID_I_CLR(r) & PARTID_D_CLR(r))
+
+#define PARTID_I_SET(r, id)		(PARTID_I_CLR(r) | ((id) << PARTID_I_SHIFT))
+#define PARTID_D_SET(r, id)		(PARTID_D_CLR(r) | ((id) << PARTID_D_SHIFT))
+#define PARTID_SET(r, id)		(PARTID_CLR(r) | ((id) << PARTID_I_SHIFT) | ((id) << PARTID_D_SHIFT))
+
+#define PMG_I_MASK			(PMG_MASK << PMG_I_SHIFT)
+#define PMG_D_MASK			(PMG_MASK << PMG_D_SHIFT)
+#define PMG_I_CLR(r)			((r) & ~PMG_I_MASK)
+#define PMG_D_CLR(r)			((r) & ~PMG_D_MASK)
+#define PMG_CLR(r)			(PMG_I_CLR(r) & PMG_D_CLR(r))
+
+#define PMG_I_SET(r, id)		(PMG_I_CLR(r) | ((id) << PMG_I_SHIFT))
+#define PMG_D_SET(r, id)		(PMG_D_CLR(r) | ((id) << PMG_D_SHIFT))
+#define PMG_SET(r, id)			(PMG_CLR(r) | ((id) << PMG_I_SHIFT) | ((id) << PMG_D_SHIFT))
+
+#define TRAPMPAM1EL1_SHIFT		(PMG_D_SHIFT + PMG_BITS)
+#define TRAPMPAM0EL1_SHIFT		(TRAPMPAM1EL1_SHIFT + 1)
+#define TRAPLOWER_SHIFT			(TRAPMPAM0EL1_SHIFT + 13)
+#define MPAMEN_SHIFT			(TRAPLOWER_SHIFT + 1)
+
+/*
+ * MPAMHCR_EL2:
+ * 0		EL0_VPMEN
+ * 1		EL1_VPMEN
+ * 7:2		Reserved
+ * 8		GSTAPP_PLK
+ * 30:9		Reserved
+ * 31		TRAP_MPAMIDR_EL1
+ * 63:32	Reserved
+ */
+#define EL0_VPMEN_SHIFT			(0)
+#define EL1_VPMEN_SHIFT			(EL0_VPMEN_SHIFT + 1)
+#define GSTAPP_PLK_SHIFT		(8)
+#define TRAP_MPAMIDR_EL1_SHIFT		(31)
+
+/*
+ * MPAMIDR_EL1:
+ * 15:0		PARTID_MAX
+ * 16		Reserved
+ * 17		HAS_HCR
+ * 20:18	VPMR_MAX
+ * 31:21	Reserved
+ * 39:32	PMG_MAX
+ * 63:40	Reserved
+ */
+#define VPMR_MAX_BITS			(3)
+#define PARTID_MAX_SHIFT		(0)
+#define HAS_HCR_SHIFT			(PARTID_MAX_SHIFT + PARTID_BITS + 1)
+#define VPMR_MAX_SHIFT			(HAS_HCR_SHIFT + 1)
+#define PMG_MAX_SHIFT			(VPMR_MAX_SHIFT + VPMR_MAX_BITS + 11)
+#define VPMR_MASK			MPAM_MASK(VPMR_MAX_BITS)
+
+/*
+ * MPAMVPMV_EL2:
+ * 31:0		VPM_V
+ * 63:32	Reserved
+ */
+#define VPM_V_BITS			32
+
+DECLARE_STATIC_KEY_FALSE(resctrl_enable_key);
+DECLARE_STATIC_KEY_FALSE(resctrl_mon_enable_key);
+
+extern bool rdt_alloc_capable;
+extern bool rdt_mon_capable;
+
+enum rdt_group_type {
+	RDTCTRL_GROUP = 0,
+	RDTMON_GROUP,
+	RDT_NUM_GROUP,
+};
+
+/**
+ * struct mongroup - store mon group's data in resctrl fs.
+ * @mon_data_kn		kernlfs node for the mon_data directory
+ * @parent:			parent rdtgrp
+ * @crdtgrp_list:		child rdtgroup node list
+ * @rmid:			rmid for this rdtgroup
+ */
+struct mongroup {
+	struct kernfs_node	*mon_data_kn;
+	struct rdtgroup		*parent;
+	struct list_head	crdtgrp_list;
+	u32			rmid;
+};
+
+/**
+ * struct rdtgroup - store rdtgroup's data in resctrl file system.
+ * @kn:				kernfs node
+ * @resctrl_group_list:		linked list for all rdtgroups
+ * @closid:			closid for this rdtgroup
+ * #endif
+ * @cpu_mask:			CPUs assigned to this rdtgroup
+ * @flags:			status bits
+ * @waitcount:			how many cpus expect to find this
+ *				group when they acquire resctrl_group_mutex
+ * @type:			indicates type of this rdtgroup - either
+ *				monitor only or ctrl_mon group
+ * @mon:			mongroup related data
+ */
+struct rdtgroup {
+	struct kernfs_node	*kn;
+	struct list_head	resctrl_group_list;
+	u32			closid;
+	struct cpumask		cpu_mask;
+	int			flags;
+	atomic_t		waitcount;
+	enum rdt_group_type	type;
+	struct mongroup		mon;
+};
+
+extern int max_name_width, max_data_width;
+
+/* rdtgroup.flags */
+#define	RDT_DELETED		1
+
+/**
+ * struct rdt_domain - group of cpus sharing an RDT resource
+ * @list:	all instances of this resource
+ * @id:		unique id for this instance
+ * @cpu_mask:	which cpus share this resource
+ * @rmid_busy_llc:
+ *		bitmap of which limbo RMIDs are above threshold
+ * @mbm_total:	saved state for MBM total bandwidth
+ * @mbm_local:	saved state for MBM local bandwidth
+ * @mbm_over:	worker to periodically read MBM h/w counters
+ * @cqm_limbo:	worker to periodically read CQM h/w counters
+ * @mbm_work_cpu:
+ *		worker cpu for MBM h/w counters
+ * @cqm_work_cpu:
+ *		worker cpu for CQM h/w counters
+ * @ctrl_val:	array of cache or mem ctrl values (indexed by CLOSID)
+ * @new_ctrl:	new ctrl value to be loaded
+ * @have_new_ctrl: did user provide new_ctrl for this domain
+ */
+struct rdt_domain {
+	struct list_head	list;
+	int			id;
+	struct cpumask		cpu_mask;
+	void __iomem		*base;
+
+	/* arch specific fields */
+	u32			*ctrl_val;
+	u32			new_ctrl;
+	bool			have_new_ctrl;
+
+	/* for debug */
+	char			*cpus_list;
+};
+
+extern struct mutex resctrl_group_mutex;
+
+extern struct resctrl_resource resctrl_resources_all[];
+
+int __init resctrl_group_init(void);
+
+enum {
+	MPAM_RESOURCE_SMMU,
+	MPAM_RESOURCE_CACHE,
+	MPAM_RESOURCE_MC,
+
+	/* Must be the last */
+	MPAM_NUM_RESOURCES,
+};
+
+void rdt_last_cmd_clear(void);
+void rdt_last_cmd_puts(const char *s);
+void rdt_last_cmd_printf(const char *fmt, ...);
+
+int alloc_rmid(void);
+void free_rmid(u32 rmid);
+int resctrl_group_mondata_show(struct seq_file *m, void *arg);
+void rmdir_mondata_subdir_allrdtgrp(struct resctrl_resource *r,
+				    unsigned int dom_id);
+void mkdir_mondata_subdir_allrdtgrp(struct resctrl_resource *r,
+				    struct rdt_domain *d);
+
+void closid_init(void);
+int closid_alloc(void);
+void closid_free(int closid);
+
+int cdp_enable(int level, int data_type, int code_type);
+void resctrl_resource_reset(void);
+void release_rdtgroupfs_options(void);
+int parse_rdtgroupfs_options(char *data);
+
+static inline int __resctrl_group_show_options(struct seq_file *seq)
+{
+	return 0;
+}
+
+void post_resctrl_mount(void);
+
+#define MPAM_SYS_REG_DEBUG
+
+#ifdef MPAM_SYS_REG_DEBUG
+static inline u64 mpam_read_sysreg_s(u64 reg, char *name)
+{
+	pr_info("cpu %2d (cur: %s(%d)): read_sysreg_s: %s (addr %016llx)\n",
+		smp_processor_id(), current->comm, current->pid, name, reg);
+	return 0;
+}
+#else
+#define mpam_read_sysreg_s(reg, name) read_sysreg_s(reg)
+#endif
+
+#ifdef MPAM_SYS_REG_DEBUG
+static inline u64 mpam_write_sysreg_s(u64 v, u64 reg, char *name)
+{
+	pr_info("cpu %2d (cur %s(%d)): write_sysreg_s: %s (addr %016llx), value %016llx\n",
+		smp_processor_id(), current->comm, current->pid, name, reg, v);
+	return 0;
+}
+#else
+#define mpam_write_sysreg_s(v, r, n) write_sysreg_s(v, r)
+#endif
+
+#ifdef MPAM_SYS_REG_DEBUG
+static inline u32 mpam_readl(const volatile void __iomem *addr)
+{
+	return pr_info("readl: %016llx\n", (u64)addr);
+}
+#else
+#define mpam_readl(addr) readl(addr)
+#endif
+
+#ifdef MPAM_SYS_REG_DEBUG
+static inline u32 mpam_writel(u64 v, const volatile void __iomem *addr)
+{
+	return pr_info("writel: %016llx to %016llx\n", v, (u64)addr);
+}
+#else
+#define mpam_writel(v, addr) writel(v, addr)
+#endif
+
+/**
+ * struct msr_param - set a range of MSRs from a domain
+ * @res:	The resource to use
+ * @value:	value
+ */
+struct msr_param {
+	struct resctrl_resource	*res;
+	u64			value;
+};
+
+/**
+ * struct resctrl_resource - attributes of an RDT resource
+ * @rid:		The index of the resource
+ * @alloc_enabled:	Is allocation enabled on this machine
+ * @mon_enabled:		Is monitoring enabled for this feature
+ * @alloc_capable:	Is allocation available on this machine
+ * @mon_capable:		Is monitor feature available on this machine
+ * @name:		Name to use in "schemata" file
+ * @num_closid:		Number of CLOSIDs available
+ * @cache_level:	Which cache level defines scope of this resource
+ * @default_ctrl:	Specifies default cache cbm or memory B/W percent.
+ * @msr_base:		Base MSR address for CBMs
+ * @msr_update:		Function pointer to update QOS MSRs
+ * @data_width:		Character width of data when displaying
+ * @domains:		All domains for this resource
+ * @cache:		Cache allocation related data
+ * @format_str:		Per resource format string to show domain value
+ * @parse_ctrlval:	Per resource function pointer to parse control values
+ * @evt_list:			List of monitoring events
+ * @num_rmid:			Number of RMIDs available
+ * @mon_scale:			cqm counter * mon_scale = occupancy in bytes
+ * @fflags:			flags to choose base and info files
+ */
+
+struct raw_resctrl_resource {
+	int			num_partid;
+	u32			default_ctrl;
+	void (*msr_update)	(struct rdt_domain *d, int partid);
+	u64  (*msr_read)	(struct rdt_domain *d, int partid);
+	int			data_width;
+	const char		*format_str;
+	int (*parse_ctrlval)	(char *buf, struct raw_resctrl_resource *r,
+				 struct rdt_domain *d);
+	int			num_pmg;
+};
+
+int parse_cbm(char *buf, struct raw_resctrl_resource *r, struct rdt_domain *d);
+
+#endif /* _ASM_ARM64_MPAM_H */
diff --git a/arch/arm64/include/asm/mpam_resource.h b/arch/arm64/include/asm/mpam_resource.h
new file mode 100644
index 000000000000..cd83322b8a43
--- /dev/null
+++ b/arch/arm64/include/asm/mpam_resource.h
@@ -0,0 +1,84 @@
+/* mpam resource: like L3, memory */
+
+#ifndef _ASM_ARM64_MPAM_RESOURCE_H
+#define _ASM_ARM64_MPAM_RESOURCE_H
+
+#include <linux/bitops.h>
+
+#define MPAMF_IDR		0x0000
+#define MPAMF_SIDR		0x0008
+#define MPAMF_MSMON_IDR		0x0080
+#define MPAMF_IMPL_IDR		0x0028
+#define MPAMF_CPOR_IDR		0x0030
+#define MPAMF_CCAP_IDR		0x0038
+#define MPAMF_MBW_IDR		0x0040
+#define MPAMF_PRI_IDR		0x0048
+#define MPAMF_CSUMON_IDR	0x0088
+#define MPAMF_MBWUMON_IDR	0x0090
+#define MPAMF_PARTID_NRW_IDR	0x0050
+#define MPAMF_IIDR		0x0018
+#define MPAMF_AIDR		0x0020
+#define MPAMCFG_PART_SEL	0x0100
+#define MPAMCFG_CPBM		0x1000
+#define MPAMCFG_CMAX		0x0108
+#define MPAMCFG_MBW_MIN		0x0200
+#define MPAMCFG_MBW_MAX		0x0208
+#define MPAMCFG_MBW_WINWD	0x0220
+#define MPAMCFG_MBW_PBM		0x2000
+#define MPAMCFG_PRI		0x0400
+#define MPAMCFG_MBW_PROP	0x0500
+#define MPAMCFG_INTPARTID	0x0600
+#define MSMON_CFG_MON_SEL	0x0800
+#define MSMON_CFG_CSU_FLT	0x0810
+#define MSMON_CFG_CSU_CTL	0x0818
+#define MSMON_CFG_MBWU_FLT	0x0820
+#define MSMON_CFG_MBWU_CTL	0x0828
+#define MSMON_CSU		0x0840
+#define MSMON_CSU_CAPTURE	0x0848
+#define MSMON_MBWU		0x0860
+#define MSMON_MBWU_CAPTURE	0x0868
+#define MSMON_CAPT_EVNT		0x0808
+#define MPAMF_ESR		0x00F8
+#define MPAMF_ECR		0x00F0
+
+#define HAS_CCAP_PART		BIT(24)
+#define HAS_CPOR_PART		BIT(25)
+#define HAS_MBW_PART		BIT(26)
+#define HAS_PRI_PART		BIT(27)
+#define HAS_IMPL_IDR		BIT(29)
+#define HAS_MSMON		BIT(30)
+
+/* MPAMF_IDR */
+/* TODO */
+
+#define CPBM_WD_MASK		0xFFFF
+#define CPBM_MASK		0x7FFF
+
+#define BWA_WD			6		/* hard code for P680 */
+#define MBW_MAX_MASK		0xFC00
+#define MBW_MAX_HARDLIM		BIT(31)
+
+/* [FIXME] hard code for hardlim */
+#define MBW_MAX_SET(v)		(MBW_MAX_HARDLIM|((v) << (15 - BWA_WD)))
+#define MBW_MAX_GET(v)		(((v) & MBW_MAX_MASK) >> (15 - BWA_WD))
+/*
+ * emulate the mpam nodes
+ * These should be reported by ACPI MPAM Table.
+ */
+
+struct mpam_node {
+	/* MPAM node header */
+	u8              type;   /* MPAM_SMMU, MPAM_CACHE, MPAM_MC */
+	u64             addr;
+	void __iomem	*base;
+	struct cpumask  cpu_mask;
+	u64		default_ctrl;
+
+	/* for debug */
+	char            *cpus_list;
+	char		*name;
+};
+
+int mpam_nodes_init(void);
+
+#endif /* _ASM_ARM64_MPAM_RESOURCE_H */
diff --git a/arch/arm64/include/asm/mpam_sched.h b/arch/arm64/include/asm/mpam_sched.h
new file mode 100644
index 000000000000..31522efbf8e5
--- /dev/null
+++ b/arch/arm64/include/asm/mpam_sched.h
@@ -0,0 +1,48 @@
+#ifndef _ASM_ARM64_MPAM_SCHED_H
+#define _ASM_ARM64_MPAM_SCHED_H
+
+#ifdef CONFIG_MPAM
+
+#include <linux/sched.h>
+#include <linux/jump_label.h>
+
+/**
+ * struct intel_pqr_state - State cache for the PQR MSR
+ * @cur_rmid:		The cached Resource Monitoring ID
+ * @cur_closid:	The cached Class Of Service ID
+ * @default_rmid:	The user assigned Resource Monitoring ID
+ * @default_closid:	The user assigned cached Class Of Service ID
+ *
+ * The upper 32 bits of IA32_PQR_ASSOC contain closid and the
+ * lower 10 bits rmid. The update to IA32_PQR_ASSOC always
+ * contains both parts, so we need to cache them. This also
+ * stores the user configured per cpu CLOSID and RMID.
+ *
+ * The cache also helps to avoid pointless updates if the value does
+ * not change.
+ */
+struct intel_pqr_state {
+	u32			cur_rmid;
+	u32			cur_closid;
+	u32			default_rmid;
+	u32			default_closid;
+};
+
+DECLARE_PER_CPU(struct intel_pqr_state, pqr_state);
+
+extern void __mpam_sched_in(void);
+DECLARE_STATIC_KEY_FALSE(resctrl_enable_key);
+
+static inline void mpam_sched_in(void)
+{
+	if (static_branch_likely(&resctrl_enable_key))
+		__mpam_sched_in();
+}
+
+#else
+
+static inline void mpam_sched_in(void) {}
+
+#endif /* CONFIG_MPAM */
+
+#endif
diff --git a/arch/arm64/include/asm/resctrl.h b/arch/arm64/include/asm/resctrl.h
new file mode 100644
index 000000000000..ffebafa85839
--- /dev/null
+++ b/arch/arm64/include/asm/resctrl.h
@@ -0,0 +1,66 @@
+#ifndef _ASM_ARM64_RESCTRL_H
+#define _ASM_ARM64_RESCTRL_H
+
+#include <asm/mpam_sched.h>
+#include <asm/mpam.h>
+
+#define resctrl_group rdtgroup
+#define resctrl_alloc_capable rdt_alloc_capable
+#define resctrl_mon_capable rdt_mon_capable
+
+static inline int alloc_mon_id(void)
+{
+
+	return alloc_rmid();
+}
+
+static inline void free_mon_id(u32 id)
+{
+	free_rmid(id);
+}
+
+void pmg_init(void);
+static inline void resctrl_id_init(void)
+{
+	closid_init();
+	pmg_init();
+}
+
+static inline int resctrl_id_alloc(void)
+{
+	return closid_alloc();
+}
+
+static inline void resctrl_id_free(int id)
+{
+	closid_free(id);
+}
+
+void update_cpu_closid_rmid(void *info);
+void update_closid_rmid(const struct cpumask *cpu_mask, struct resctrl_group *r);
+int __resctrl_group_move_task(struct task_struct *tsk,
+				struct resctrl_group *rdtgrp);
+
+ssize_t resctrl_group_schemata_write(struct kernfs_open_file *of,
+				char *buf, size_t nbytes, loff_t off);
+
+int resctrl_group_schemata_show(struct kernfs_open_file *of,
+				struct seq_file *s, void *v);
+
+#define release_resctrl_group_fs_options release_rdtgroupfs_options
+#define parse_resctrl_group_fs_options parse_rdtgroupfs_options
+
+#define for_each_resctrl_resource(r)					\
+	for (r = resctrl_resources_all;					\
+	     r < resctrl_resources_all + MPAM_NUM_RESOURCES;		\
+	     r++)							\
+
+int mkdir_mondata_all(struct kernfs_node *parent_kn,
+			     struct resctrl_group *prgrp,
+			     struct kernfs_node **dest_kn);
+
+int
+mongroup_create_dir(struct kernfs_node *parent_kn, struct resctrl_group *prgrp,
+		    char *name, struct kernfs_node **dest_kn);
+
+#endif /* _ASM_ARM64_RESCTRL_H */
diff --git a/arch/arm64/kernel/Makefile b/arch/arm64/kernel/Makefile
index d970862ec648..1f53609efa9a 100644
--- a/arch/arm64/kernel/Makefile
+++ b/arch/arm64/kernel/Makefile
@@ -66,6 +66,7 @@ obj-$(CONFIG_SDEI_WATCHDOG)		+= watchdog_sdei.o
 obj-$(CONFIG_ARM64_PTR_AUTH)		+= pointer_auth.o
 obj-$(CONFIG_SHADOW_CALL_STACK)		+= scs.o
 obj-$(CONFIG_ARM64_MTE)			+= mte.o
+obj-$(CONFIG_MPAM)			+= mpam.o mpam_ctrlmon.o mpam_mon.o mpam_resource.o
 
 obj-y					+= vdso/ probes/
 obj-$(CONFIG_COMPAT_VDSO)		+= vdso32/
diff --git a/arch/arm64/kernel/mpam.c b/arch/arm64/kernel/mpam.c
new file mode 100644
index 000000000000..56188953f954
--- /dev/null
+++ b/arch/arm64/kernel/mpam.c
@@ -0,0 +1,1088 @@
+/*
+ * Resource Director Technology(RDT)
+ * - Cache Allocation code.
+ *
+ * Copyright (C) 2016 Intel Corporation
+ *
+ * Authors:
+ *    Fenghua Yu <fenghua.yu@intel.com>
+ *    Tony Luck <tony.luck@intel.com>
+ *    Vikas Shivappa <vikas.shivappa@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * More information about RDT be found in the Intel (R) x86 Architecture
+ * Software Developer Manual June 2016, volume 3, section 17.17.
+ */
+
+#define pr_fmt(fmt)	KBUILD_MODNAME ": " fmt
+
+#include <linux/slab.h>
+#include <linux/err.h>
+#include <linux/cacheinfo.h>
+#include <linux/cpuhotplug.h>
+#include <linux/task_work.h>
+#include <linux/sched/signal.h>
+#include <linux/sched/task.h>
+#include <linux/resctrlfs.h>
+
+#include <asm/mpam_sched.h>
+#include <asm/mpam_resource.h>
+#include <asm/resctrl.h>
+#include <asm/io.h>
+
+/* Mutex to protect rdtgroup access. */
+DEFINE_MUTEX(resctrl_group_mutex);
+
+/*
+ * The cached intel_pqr_state is strictly per CPU and can never be
+ * updated from a remote CPU. Functions which modify the state
+ * are called with interrupts disabled and no preemption, which
+ * is sufficient for the protection.
+ */
+DEFINE_PER_CPU(struct intel_pqr_state, pqr_state);
+
+/*
+ * Used to store the max resource name width and max resource data width
+ * to display the schemata in a tabular format
+ */
+int max_name_width, max_data_width;
+
+/*
+ * Global boolean for rdt_alloc which is true if any
+ * resource allocation is enabled.
+ */
+bool rdt_alloc_capable;
+
+char *mpam_types_str[] = {
+	"MPAM_RESOURCE_SMMU",
+	"MPAM_RESOURCE_CACHE",
+	"MPAM_RESOURCE_MC",
+};
+
+struct mpam_node mpam_node_all[] = {
+	/* P0 DIE 0: cluster 0 */
+	{
+		.name			= "L3T0",
+		.type                   = MPAM_RESOURCE_CACHE,
+		.addr                   = 0x90390000,
+		.cpus_list              = "0",
+		.default_ctrl		= 0x7fff,
+	},
+
+	/* P0 DIE 0: cluster 1 */
+	{
+		.name			= "L3T1",
+		.type                   = MPAM_RESOURCE_CACHE,
+		.addr                   = 0x903a0000,
+		.cpus_list              = "1",
+		.default_ctrl		= 0x7fff,
+	},
+
+	/* P0 DIE 0: cluster 2 */
+	{
+		.name			= "L3T2",
+		.type                   = MPAM_RESOURCE_CACHE,
+		.addr                   = 0x903b0000,
+		.cpus_list              = "2",
+		.default_ctrl		= 0x7fff,
+	},
+
+	/* P0 DIE 0: cluster 3 */
+	{
+		.name			= "L3T3",
+		.type                   = MPAM_RESOURCE_CACHE,
+		.addr                   = 0x903c0000,
+		.cpus_list              = "3",
+		.default_ctrl		= 0x7fff,
+	},
+
+	/* P0 DIE 0: HHA0 */
+	{
+		.name			= "HHA0",
+		.type                   = MPAM_RESOURCE_MC,
+		.addr                   = 0x90410000,
+		.cpus_list              = "0-3",
+	},
+
+	/* P0 DIE 0: HHA1 */
+	{
+		.name			= "HHA1",
+		.type                   = MPAM_RESOURCE_MC,
+		.addr                   = 0x90420000,
+		.cpus_list              = "0-3",
+	},
+	/* other mpam nodes ... */
+};
+
+int mpam_nodes_init(void)
+{
+	int i, ret = 0;
+	size_t num_nodes = ARRAY_SIZE(mpam_node_all);
+	struct mpam_node *n;
+
+	for (i = 0; i < num_nodes; i++) {
+		n = &mpam_node_all[i];
+		ret |= cpulist_parse(n->cpus_list, &n->cpu_mask);
+		n->base = ioremap(n->addr, 0x10000);
+	}
+
+	return ret;
+}
+
+void mpam_nodes_show(void)
+{
+	int i, cpu;
+	size_t num_nodes = ARRAY_SIZE(mpam_node_all);
+	struct mpam_node *n;
+
+	char *types[] = {"MPAM_RESOURCE_SMMU", "MPAM_RESOURCE_CACHE", "MPAM_RESOURCE_MC"};
+
+	for (i = 0; i < num_nodes; i++) {
+		n = &mpam_node_all[i];
+		pr_cont("%s: type: %s; addr = %016llx; base = %016llx; cpus_list = %s; cpus: ",
+			__func__, types[n->type], n->addr, (u64)n->base, n->cpus_list);
+
+		for_each_cpu(cpu, &n->cpu_mask) {
+			pr_cont("%d, ", cpu);
+		}
+		pr_cont("\n");
+	}
+}
+
+static void
+cat_wrmsr(struct rdt_domain *d, int partid);
+static void
+bw_wrmsr(struct rdt_domain *d, int partid);
+
+u64 cat_rdmsr(struct rdt_domain *d, int partid);
+u64 bw_rdmsr(struct rdt_domain *d, int partid);
+
+#define domain_init(id) LIST_HEAD_INIT(resctrl_resources_all[id].domains)
+
+struct raw_resctrl_resource raw_resctrl_resources_all[] = {
+	[MPAM_RESOURCE_SMMU] = {
+		.msr_update		= cat_wrmsr,
+		.msr_read		= cat_rdmsr,
+		.parse_ctrlval		= parse_cbm,
+		.format_str		= "%d=%0*x",
+	},
+	[MPAM_RESOURCE_CACHE] = {
+		.msr_update		= cat_wrmsr,
+		.msr_read		= cat_rdmsr,
+		.parse_ctrlval		= parse_cbm,
+		.format_str		= "%d=%0*x",
+	},
+	[MPAM_RESOURCE_MC] = {
+		.msr_update		= bw_wrmsr,
+		.msr_read		= bw_rdmsr,
+		.parse_ctrlval		= parse_cbm,	/* [FIXME] add parse_bw() helper */
+		.format_str		= "%d=%0*x",
+	},
+};
+
+struct resctrl_resource resctrl_resources_all[] = {
+	[MPAM_RESOURCE_SMMU] = {
+		.rid			= MPAM_RESOURCE_SMMU,
+		.name			= "SMMU",
+		.domains		= domain_init(MPAM_RESOURCE_SMMU),
+		.res			= &raw_resctrl_resources_all[MPAM_RESOURCE_SMMU],
+		.fflags			= RFTYPE_RES_SMMU,
+		.alloc_enabled		= 1,
+	},
+	[MPAM_RESOURCE_CACHE] = {
+		.rid			= MPAM_RESOURCE_CACHE,
+		.name			= "L3",
+		.domains		= domain_init(MPAM_RESOURCE_CACHE),
+		.res			= &raw_resctrl_resources_all[MPAM_RESOURCE_CACHE],
+		.fflags			= RFTYPE_RES_CACHE,
+		.alloc_enabled		= 1,
+	},
+	[MPAM_RESOURCE_MC] = {
+		.rid			= MPAM_RESOURCE_MC,
+		.name			= "MB",
+		.domains		= domain_init(MPAM_RESOURCE_MC),
+		.res			= &raw_resctrl_resources_all[MPAM_RESOURCE_MC],
+		.fflags			= RFTYPE_RES_MC,
+		.alloc_enabled		= 1,
+	},
+};
+
+static void
+cat_wrmsr(struct rdt_domain *d, int partid)
+{
+	mpam_writel(partid, d->base + MPAMCFG_PART_SEL);
+	mpam_writel(d->ctrl_val[partid], d->base + MPAMCFG_CPBM);
+}
+
+static void
+bw_wrmsr(struct rdt_domain *d, int partid)
+{
+	u64 val = MBW_MAX_SET(d->ctrl_val[partid]);
+
+	mpam_writel(partid, d->base + MPAMCFG_PART_SEL);
+	mpam_writel(val, d->base + MPAMCFG_MBW_MAX);
+}
+
+u64 cat_rdmsr(struct rdt_domain *d, int partid)
+{
+	mpam_writel(partid, d->base + MPAMCFG_PART_SEL);
+	return mpam_readl(d->base + MPAMCFG_CPBM);
+}
+
+u64 bw_rdmsr(struct rdt_domain *d, int partid)
+{
+	u64 max;
+
+	mpam_writel(partid, d->base + MPAMCFG_PART_SEL);
+	max = mpam_readl(d->base + MPAMCFG_MBW_MAX);
+
+	return MBW_MAX_GET(max);
+}
+
+/*
+ * Trivial allocator for CLOSIDs. Since h/w only supports a small number,
+ * we can keep a bitmap of free CLOSIDs in a single integer.
+ *
+ * Using a global CLOSID across all resources has some advantages and
+ * some drawbacks:
+ * + We can simply set "current->closid" to assign a task to a resource
+ *   group.
+ * + Context switch code can avoid extra memory references deciding which
+ *   CLOSID to load into the PQR_ASSOC MSR
+ * - We give up some options in configuring resource groups across multi-socket
+ *   systems.
+ * - Our choices on how to configure each resource become progressively more
+ *   limited as the number of resources grows.
+ */
+static int closid_free_map;
+
+void closid_init(void)
+{
+	int resctrl_min_closid = 32;
+
+	closid_free_map = BIT_MASK(resctrl_min_closid) - 1;
+
+	/* CLOSID 0 is always reserved for the default group */
+	closid_free_map &= ~1;
+}
+
+int closid_alloc(void)
+{
+	u32 closid = ffs(closid_free_map);
+
+	if (closid == 0)
+		return -ENOSPC;
+	closid--;
+	closid_free_map &= ~(1 << closid);
+
+	return closid;
+}
+
+void closid_free(int closid)
+{
+	closid_free_map |= 1 << closid;
+}
+
+static int mpam_online_cpu(unsigned int cpu)
+{
+	cpumask_set_cpu(cpu, &resctrl_group_default.cpu_mask);
+	return 0;
+}
+
+/* [FIXME] remove related resource when cpu offline */
+static int mpam_offline_cpu(unsigned int cpu)
+{
+	pr_info("offline cpu\n");
+	return 0;
+}
+
+/*
+ * Choose a width for the resource name and resource data based on the
+ * resource that has widest name and cbm.
+ */
+static __init void mpam_init_padding(void)
+{
+	struct resctrl_resource *r;
+	struct raw_resctrl_resource *rr;
+	int cl;
+
+	for_each_resctrl_resource(r) {
+		if (r->alloc_enabled) {
+			rr = (struct raw_resctrl_resource *)r->res;
+			cl = strlen(r->name);
+			if (cl > max_name_width)
+				max_name_width = cl;
+
+			if (rr->data_width > max_data_width)
+				max_data_width = rr->data_width;
+		}
+	}
+}
+
+static __init bool get_rdt_alloc_resources(void)
+{
+	bool ret = true;
+
+	return ret;
+}
+
+static __init bool get_rdt_mon_resources(void)
+{
+	bool ret = true;
+
+	return ret;
+}
+
+static __init bool get_resctrl_resources(void)
+{
+	rdt_alloc_capable = get_rdt_alloc_resources();
+	rdt_mon_capable = get_rdt_mon_resources();
+
+	return (rdt_mon_capable || rdt_alloc_capable);
+}
+
+void post_resctrl_mount(void)
+{
+	if (rdt_alloc_capable)
+		static_branch_enable_cpuslocked(&resctrl_alloc_enable_key);
+	if (rdt_mon_capable)
+		static_branch_enable_cpuslocked(&resctrl_mon_enable_key);
+
+	if (rdt_alloc_capable || rdt_mon_capable)
+		static_branch_enable_cpuslocked(&resctrl_enable_key);
+}
+
+static int reset_all_ctrls(struct resctrl_resource *r)
+{
+	pr_info("%s\n", __func__);
+	return 0;
+}
+
+void resctrl_resource_reset(void)
+{
+	struct resctrl_resource *r;
+
+	/*Put everything back to default values. */
+	for_each_resctrl_resource(r) {
+		if (r->alloc_enabled)
+			reset_all_ctrls(r);
+	}
+}
+
+void release_rdtgroupfs_options(void)
+{
+}
+
+int parse_rdtgroupfs_options(char *data)
+{
+	int ret = 0;
+
+	pr_err("Invalid mount option\n");
+
+	return ret;
+}
+
+/*
+ * This is safe against intel_resctrl_sched_in() called from __switch_to()
+ * because __switch_to() is executed with interrupts disabled. A local call
+ * from update_closid_rmid() is proteced against __switch_to() because
+ * preemption is disabled.
+ */
+void update_cpu_closid_rmid(void *info)
+{
+	struct rdtgroup *r = info;
+
+	if (r) {
+		this_cpu_write(pqr_state.default_closid, r->closid);
+		this_cpu_write(pqr_state.default_rmid, r->mon.rmid);
+	}
+
+	/*
+	 * We cannot unconditionally write the MSR because the current
+	 * executing task might have its own closid selected. Just reuse
+	 * the context switch code.
+	 */
+	mpam_sched_in();
+}
+
+/*
+ * Update the PGR_ASSOC MSR on all cpus in @cpu_mask,
+ *
+ * Per task closids/rmids must have been set up before calling this function.
+ */
+void
+update_closid_rmid(const struct cpumask *cpu_mask, struct rdtgroup *r)
+{
+	int cpu = get_cpu();
+
+	if (cpumask_test_cpu(cpu, cpu_mask))
+		update_cpu_closid_rmid(r);
+	smp_call_function_many(cpu_mask, update_cpu_closid_rmid, r, 1);
+	put_cpu();
+}
+
+struct task_move_callback {
+	struct callback_head	work;
+	struct rdtgroup		*rdtgrp;
+};
+
+static void move_myself(struct callback_head *head)
+{
+	struct task_move_callback *callback;
+	struct rdtgroup *rdtgrp;
+
+	callback = container_of(head, struct task_move_callback, work);
+	rdtgrp = callback->rdtgrp;
+
+	/*
+	 * If resource group was deleted before this task work callback
+	 * was invoked, then assign the task to root group and free the
+	 * resource group.
+	 */
+	if (atomic_dec_and_test(&rdtgrp->waitcount) &&
+	    (rdtgrp->flags & RDT_DELETED)) {
+		current->closid = 0;
+		current->rmid = 0;
+		kfree(rdtgrp);
+	}
+
+	preempt_disable();
+	/* update PQR_ASSOC MSR to make resource group go into effect */
+	mpam_sched_in();
+	preempt_enable();
+
+	kfree(callback);
+}
+
+int __resctrl_group_move_task(struct task_struct *tsk,
+				struct rdtgroup *rdtgrp)
+{
+	struct task_move_callback *callback;
+	int ret;
+
+	callback = kzalloc(sizeof(*callback), GFP_KERNEL);
+	if (!callback)
+		return -ENOMEM;
+	callback->work.func = move_myself;
+	callback->rdtgrp = rdtgrp;
+
+	/*
+	 * Take a refcount, so rdtgrp cannot be freed before the
+	 * callback has been invoked.
+	 */
+	atomic_inc(&rdtgrp->waitcount);
+	ret = task_work_add(tsk, &callback->work, true);
+	if (ret) {
+		/*
+		 * Task is exiting. Drop the refcount and free the callback.
+		 * No need to check the refcount as the group cannot be
+		 * deleted before the write function unlocks resctrl_group_mutex.
+		 */
+		atomic_dec(&rdtgrp->waitcount);
+		kfree(callback);
+		rdt_last_cmd_puts("task exited\n");
+	} else {
+		/*
+		 * For ctrl_mon groups move both closid and rmid.
+		 * For monitor groups, can move the tasks only from
+		 * their parent CTRL group.
+		 */
+		if (rdtgrp->type == RDTCTRL_GROUP) {
+			tsk->closid = rdtgrp->closid;
+			tsk->rmid = rdtgrp->mon.rmid;
+		} else if (rdtgrp->type == RDTMON_GROUP) {
+			if (rdtgrp->mon.parent->closid == tsk->closid) {
+				tsk->rmid = rdtgrp->mon.rmid;
+			} else {
+				rdt_last_cmd_puts("Can't move task to different control group\n");
+				ret = -EINVAL;
+			}
+		}
+	}
+	return ret;
+}
+
+static int resctrl_group_seqfile_show(struct seq_file *m, void *arg)
+{
+	struct kernfs_open_file *of = m->private;
+	struct rftype *rft = of->kn->priv;
+
+	if (rft->seq_show)
+		return rft->seq_show(of, m, arg);
+	return 0;
+}
+
+static ssize_t resctrl_group_file_write(struct kernfs_open_file *of, char *buf,
+				   size_t nbytes, loff_t off)
+{
+	struct rftype *rft = of->kn->priv;
+
+	if (rft->write)
+		return rft->write(of, buf, nbytes, off);
+
+	return -EINVAL;
+}
+
+struct kernfs_ops resctrl_group_kf_single_ops = {
+	.atomic_write_len	= PAGE_SIZE,
+	.write			= resctrl_group_file_write,
+	.seq_show		= resctrl_group_seqfile_show,
+};
+
+static bool is_cpu_list(struct kernfs_open_file *of)
+{
+	struct rftype *rft = of->kn->priv;
+
+	return rft->flags & RFTYPE_FLAGS_CPUS_LIST;
+}
+
+static int resctrl_group_cpus_show(struct kernfs_open_file *of,
+			      struct seq_file *s, void *v)
+{
+	struct rdtgroup *rdtgrp;
+	int ret = 0;
+
+	rdtgrp = resctrl_group_kn_lock_live(of->kn);
+
+	if (rdtgrp) {
+		seq_printf(s, is_cpu_list(of) ? "%*pbl\n" : "%*pb\n",
+			   cpumask_pr_args(&rdtgrp->cpu_mask));
+	} else {
+		ret = -ENOENT;
+	}
+	resctrl_group_kn_unlock(of->kn);
+
+	return ret;
+}
+
+static void cpumask_rdtgrp_clear(struct rdtgroup *r, struct cpumask *m)
+{
+	struct rdtgroup *crgrp;
+
+	cpumask_andnot(&r->cpu_mask, &r->cpu_mask, m);
+	/* update the child mon group masks as well*/
+	list_for_each_entry(crgrp, &r->mon.crdtgrp_list, mon.crdtgrp_list)
+		cpumask_and(&crgrp->cpu_mask, &r->cpu_mask, &crgrp->cpu_mask);
+}
+
+int cpus_ctrl_write(struct rdtgroup *rdtgrp, cpumask_var_t newmask,
+			   cpumask_var_t tmpmask, cpumask_var_t tmpmask1)
+{
+	struct rdtgroup *r, *crgrp;
+	struct list_head *head;
+
+	/* Check whether cpus are dropped from this group */
+	cpumask_andnot(tmpmask, &rdtgrp->cpu_mask, newmask);
+	if (cpumask_weight(tmpmask)) {
+		/* Can't drop from default group */
+		if (rdtgrp == &resctrl_group_default) {
+			rdt_last_cmd_puts("Can't drop CPUs from default group\n");
+			return -EINVAL;
+		}
+
+		/* Give any dropped cpus to rdtgroup_default */
+		cpumask_or(&resctrl_group_default.cpu_mask,
+				&resctrl_group_default.cpu_mask, tmpmask);
+		update_closid_rmid(tmpmask, &resctrl_group_default);
+	}
+
+	/*
+	 * If we added cpus, remove them from previous group and
+	 * the prev group's child groups that owned them
+	 * and update per-cpu closid/rmid.
+	 */
+	cpumask_andnot(tmpmask, newmask, &rdtgrp->cpu_mask);
+	if (cpumask_weight(tmpmask)) {
+		list_for_each_entry(r, &resctrl_all_groups, resctrl_group_list) {
+			if (r == rdtgrp)
+				continue;
+			cpumask_and(tmpmask1, &r->cpu_mask, tmpmask);
+			if (cpumask_weight(tmpmask1))
+				cpumask_rdtgrp_clear(r, tmpmask1);
+		}
+		update_closid_rmid(tmpmask, rdtgrp);
+	}
+
+	/* Done pushing/pulling - update this group with new mask */
+	cpumask_copy(&rdtgrp->cpu_mask, newmask);
+
+	/*
+	 * Clear child mon group masks since there is a new parent mask
+	 * now and update the rmid for the cpus the child lost.
+	 */
+	head = &rdtgrp->mon.crdtgrp_list;
+	list_for_each_entry(crgrp, head, mon.crdtgrp_list) {
+		cpumask_and(tmpmask, &rdtgrp->cpu_mask, &crgrp->cpu_mask);
+		update_closid_rmid(tmpmask, rdtgrp);
+		cpumask_clear(&crgrp->cpu_mask);
+	}
+
+	return 0;
+}
+
+int cpus_mon_write(struct rdtgroup *rdtgrp, cpumask_var_t newmask,
+			  cpumask_var_t tmpmask)
+{
+	return 0;
+}
+
+static ssize_t resctrl_group_cpus_write(struct kernfs_open_file *of,
+				   char *buf, size_t nbytes, loff_t off)
+{
+	cpumask_var_t tmpmask, newmask, tmpmask1;
+	struct rdtgroup *rdtgrp;
+	int ret;
+
+	if (!buf)
+		return -EINVAL;
+
+	if (!zalloc_cpumask_var(&tmpmask, GFP_KERNEL))
+		return -ENOMEM;
+	if (!zalloc_cpumask_var(&newmask, GFP_KERNEL)) {
+		free_cpumask_var(tmpmask);
+		return -ENOMEM;
+	}
+	if (!zalloc_cpumask_var(&tmpmask1, GFP_KERNEL)) {
+		free_cpumask_var(tmpmask);
+		free_cpumask_var(newmask);
+		return -ENOMEM;
+	}
+
+	rdtgrp = resctrl_group_kn_lock_live(of->kn);
+	rdt_last_cmd_clear();
+	if (!rdtgrp) {
+		ret = -ENOENT;
+		rdt_last_cmd_puts("directory was removed\n");
+		goto unlock;
+	}
+
+	if (is_cpu_list(of))
+		ret = cpulist_parse(buf, newmask);
+	else
+		ret = cpumask_parse(buf, newmask);
+
+	if (ret) {
+		rdt_last_cmd_puts("bad cpu list/mask\n");
+		goto unlock;
+	}
+
+	/* check that user didn't specify any offline cpus */
+	cpumask_andnot(tmpmask, newmask, cpu_online_mask);
+	if (cpumask_weight(tmpmask)) {
+		ret = -EINVAL;
+		rdt_last_cmd_puts("can only assign online cpus\n");
+		goto unlock;
+	}
+
+	if (rdtgrp->type == RDTCTRL_GROUP)
+		ret = cpus_ctrl_write(rdtgrp, newmask, tmpmask, tmpmask1);
+	else if (rdtgrp->type == RDTMON_GROUP)
+		ret = cpus_mon_write(rdtgrp, newmask, tmpmask);
+	else
+		ret = -EINVAL;
+
+unlock:
+	resctrl_group_kn_unlock(of->kn);
+	free_cpumask_var(tmpmask);
+	free_cpumask_var(newmask);
+	free_cpumask_var(tmpmask1);
+
+	return ret ?: nbytes;
+}
+
+
+static int resctrl_group_task_write_permission(struct task_struct *task,
+					  struct kernfs_open_file *of)
+{
+	const struct cred *tcred = get_task_cred(task);
+	const struct cred *cred = current_cred();
+	int ret = 0;
+
+	/*
+	 * Even if we're attaching all tasks in the thread group, we only
+	 * need to check permissions on one of them.
+	 */
+	if (!uid_eq(cred->euid, GLOBAL_ROOT_UID) &&
+	    !uid_eq(cred->euid, tcred->uid) &&
+	    !uid_eq(cred->euid, tcred->suid)) {
+		rdt_last_cmd_printf("No permission to move task %d\n", task->pid);
+		ret = -EPERM;
+	}
+
+	put_cred(tcred);
+	return ret;
+}
+
+static int resctrl_group_move_task(pid_t pid, struct rdtgroup *rdtgrp,
+			      struct kernfs_open_file *of)
+{
+	struct task_struct *tsk;
+	int ret;
+
+	rcu_read_lock();
+	if (pid) {
+		tsk = find_task_by_vpid(pid);
+		if (!tsk) {
+			rcu_read_unlock();
+			rdt_last_cmd_printf("No task %d\n", pid);
+			return -ESRCH;
+		}
+	} else {
+		tsk = current;
+	}
+
+	get_task_struct(tsk);
+	rcu_read_unlock();
+
+	ret = resctrl_group_task_write_permission(tsk, of);
+	if (!ret)
+		ret = __resctrl_group_move_task(tsk, rdtgrp);
+
+	put_task_struct(tsk);
+	return ret;
+}
+
+static struct seq_buf last_cmd_status;
+static char last_cmd_status_buf[512];
+
+void rdt_last_cmd_clear(void)
+{
+	lockdep_assert_held(&resctrl_group_mutex);
+	seq_buf_clear(&last_cmd_status);
+}
+
+void rdt_last_cmd_puts(const char *s)
+{
+	lockdep_assert_held(&resctrl_group_mutex);
+	seq_buf_puts(&last_cmd_status, s);
+}
+
+void rdt_last_cmd_printf(const char *fmt, ...)
+{
+	va_list ap;
+
+	va_start(ap, fmt);
+	lockdep_assert_held(&resctrl_group_mutex);
+	seq_buf_vprintf(&last_cmd_status, fmt, ap);
+	va_end(ap);
+}
+
+static int rdt_last_cmd_status_show(struct kernfs_open_file *of,
+				    struct seq_file *seq, void *v)
+{
+	int len;
+
+	mutex_lock(&resctrl_group_mutex);
+	len = seq_buf_used(&last_cmd_status);
+	if (len)
+		seq_printf(seq, "%.*s", len, last_cmd_status_buf);
+	else
+		seq_puts(seq, "ok\n");
+	mutex_unlock(&resctrl_group_mutex);
+	return 0;
+}
+
+static ssize_t resctrl_group_tasks_write(struct kernfs_open_file *of,
+				    char *buf, size_t nbytes, loff_t off)
+{
+	struct rdtgroup *rdtgrp;
+	int ret = 0;
+	pid_t pid;
+
+	if (kstrtoint(strstrip(buf), 0, &pid) || pid < 0)
+		return -EINVAL;
+	rdtgrp = resctrl_group_kn_lock_live(of->kn);
+	rdt_last_cmd_clear();
+
+	if (rdtgrp)
+		ret = resctrl_group_move_task(pid, rdtgrp, of);
+	else
+		ret = -ENOENT;
+
+	resctrl_group_kn_unlock(of->kn);
+
+	return ret ?: nbytes;
+}
+
+static void show_resctrl_tasks(struct rdtgroup *r, struct seq_file *s)
+{
+	struct task_struct *p, *t;
+
+	rcu_read_lock();
+	for_each_process_thread(p, t) {
+		if ((r->type == RDTCTRL_GROUP && t->closid == r->closid) ||
+		    (r->type == RDTMON_GROUP && t->rmid == r->mon.rmid))
+			seq_printf(s, "%d: partid = %d, pmg = %d\n",
+				   t->pid, t->closid, t->rmid);
+	}
+	rcu_read_unlock();
+}
+
+static int resctrl_group_tasks_show(struct kernfs_open_file *of,
+			       struct seq_file *s, void *v)
+{
+	struct rdtgroup *rdtgrp;
+	int ret = 0;
+
+	rdtgrp = resctrl_group_kn_lock_live(of->kn);
+	if (rdtgrp)
+		show_resctrl_tasks(rdtgrp, s);
+	else
+		ret = -ENOENT;
+	resctrl_group_kn_unlock(of->kn);
+
+	return ret;
+}
+
+/* rdtgroup information files for one cache resource. */
+static struct rftype res_specific_files[] = {
+	{
+		.name		= "last_cmd_status",
+		.mode		= 0444,
+		.kf_ops		= &resctrl_group_kf_single_ops,
+		.seq_show	= rdt_last_cmd_status_show,
+		.fflags		= RF_TOP_INFO,
+	},
+	{
+		.name		= "cpus",
+		.mode		= 0644,
+		.kf_ops		= &resctrl_group_kf_single_ops,
+		.write		= resctrl_group_cpus_write,
+		.seq_show	= resctrl_group_cpus_show,
+		.fflags		= RFTYPE_BASE,
+	},
+	{
+		.name		= "cpus_list",
+		.mode		= 0644,
+		.kf_ops		= &resctrl_group_kf_single_ops,
+		.write		= resctrl_group_cpus_write,
+		.seq_show	= resctrl_group_cpus_show,
+		.flags		= RFTYPE_FLAGS_CPUS_LIST,
+		.fflags		= RFTYPE_BASE,
+	},
+	{
+		.name		= "tasks",
+		.mode		= 0644,
+		.kf_ops		= &resctrl_group_kf_single_ops,
+		.write		= resctrl_group_tasks_write,
+		.seq_show	= resctrl_group_tasks_show,
+		.fflags		= RFTYPE_BASE,
+	},
+	{
+		.name		= "schemata",
+		.mode		= 0644,
+		.kf_ops		= &resctrl_group_kf_single_ops,
+		.write		= resctrl_group_schemata_write,
+		.seq_show	= resctrl_group_schemata_show,
+		.fflags		= RF_CTRL_BASE,
+	},
+};
+
+struct rdt_domain *mpam_find_domain(struct resctrl_resource *r, int id,
+		struct list_head **pos)
+{
+	struct rdt_domain *d;
+	struct list_head *l;
+
+	if (id < 0)
+		return ERR_PTR(id);
+
+	list_for_each(l, &r->domains) {
+		d = list_entry(l, struct rdt_domain, list);
+		/* When id is found, return its domain. */
+		if (id == d->id)
+			return d;
+		/* Stop searching when finding id's position in sorted list. */
+		if (id < d->id)
+			break;
+	}
+
+	if (pos)
+		*pos = l;
+
+	return NULL;
+}
+
+static void mpam_domains_init(struct resctrl_resource *r)
+{
+	int i, cpu, id = 0;
+	size_t num_nodes = ARRAY_SIZE(mpam_node_all);
+	struct mpam_node *n;
+	struct list_head *add_pos = NULL, *l;
+	struct rdt_domain *d;
+	struct raw_resctrl_resource *rr = (struct raw_resctrl_resource *)r->res;
+
+	char *types[] = {"MPAM_RESOURCE_SMMU", "MPAM_RESOURCE_CACHE", "MPAM_RESOURCE_MC"};
+
+	for (i = 0; i < num_nodes; i++) {
+		n = &mpam_node_all[i];
+		if (r->rid != n->type)
+			continue;
+
+		pr_cont("%s: type: %s; addr = %016llx; base = %016llx; cpus_list = %s; cpus: ",
+			__func__, types[n->type], n->addr, (u64)n->base, n->cpus_list);
+
+		for_each_cpu(cpu, &n->cpu_mask) {
+			pr_cont("%d, ", cpu);
+		}
+		pr_cont("\n");
+
+		d = mpam_find_domain(r, id, &add_pos);
+		if (IS_ERR(d)) {
+			pr_warn("Could't find cache id for cpu %d\n", cpu);
+			return;
+		}
+
+		if (!d)
+			d = kzalloc(sizeof(*d), GFP_KERNEL);
+
+		if (!d)
+			return;
+
+		d->id = id;
+		d->base = n->base;
+		cpumask_copy(&d->cpu_mask, &n->cpu_mask);
+		rr->default_ctrl = n->default_ctrl;
+		rr->num_partid = 32;
+
+		d->cpus_list = n->cpus_list;
+
+		d->ctrl_val = kmalloc_array(rr->num_partid, sizeof(*d->ctrl_val), GFP_KERNEL);
+		if (!d->ctrl_val)
+			return;
+
+		list_add_tail(&d->list, add_pos);
+
+		id++;
+	}
+
+	/*
+	 * for debug
+	 */
+	list_for_each(l, &r->domains) {
+		d = list_entry(l, struct rdt_domain, list);
+
+		pr_cont("domain: %d; type: %s; base = %016llx; cpus_list = %s; cpus: ",
+			d->id, types[r->rid], (u64)d->base, d->cpus_list);
+
+		for_each_cpu(cpu, &d->cpu_mask) {
+			pr_cont("%d, ", cpu);
+		}
+		pr_cont("\n");
+	}
+}
+
+static int __init mpam_late_init(void)
+{
+	struct resctrl_resource *r;
+	int state, ret;
+
+	if (!get_resctrl_resources())
+		return -ENODEV;
+
+	mpam_init_padding();
+
+	ret = mpam_nodes_init();
+	if (ret) {
+		pr_err("internal error: bad cpu list\n");
+		return ret;
+	}
+
+	/* for debug */
+	mpam_nodes_show();
+
+	mpam_domains_init(&resctrl_resources_all[MPAM_RESOURCE_CACHE]);
+	mpam_domains_init(&resctrl_resources_all[MPAM_RESOURCE_MC]);
+
+	state = cpuhp_setup_state(CPUHP_AP_ONLINE_DYN,
+				  "arm64/mpam:online:",
+				  mpam_online_cpu, mpam_offline_cpu);
+	if (state < 0)
+		return state;
+
+	register_resctrl_specific_files(res_specific_files, ARRAY_SIZE(res_specific_files));
+
+	seq_buf_init(&last_cmd_status, last_cmd_status_buf,
+		     sizeof(last_cmd_status_buf));
+
+	ret = resctrl_group_init();
+	if (ret) {
+		cpuhp_remove_state(state);
+		return ret;
+	}
+
+	for_each_resctrl_resource(r) {
+		if (r->alloc_capable)
+			pr_info("MPAM %s allocation detected\n", r->name);
+	}
+
+	for_each_resctrl_resource(r) {
+		if (r->mon_capable)
+			pr_info("MPAM %s monitoring detected\n", r->name);
+	}
+
+	return 0;
+}
+
+late_initcall(mpam_late_init);
+
+/*
+ * __intel_rdt_sched_in() - Writes the task's CLOSid/RMID to IA32_PQR_MSR
+ *
+ * Following considerations are made so that this has minimal impact
+ * on scheduler hot path:
+ * - This will stay as no-op unless we are running on an Intel SKU
+ *   which supports resource control or monitoring and we enable by
+ *   mounting the resctrl file system.
+ * - Caches the per cpu CLOSid/RMID values and does the MSR write only
+ *   when a task with a different CLOSid/RMID is scheduled in.
+ * - We allocate RMIDs/CLOSids globally in order to keep this as
+ *   simple as possible.
+ * Must be called with preemption disabled.
+ */
+void __mpam_sched_in(void)
+{
+	struct intel_pqr_state *state = this_cpu_ptr(&pqr_state);
+	u64 partid = state->default_closid;
+	u64 pmg = state->default_rmid;
+
+	/*
+	 * If this task has a closid/rmid assigned, use it.
+	 * Else use the closid/rmid assigned to this cpu.
+	 */
+	if (static_branch_likely(&resctrl_alloc_enable_key)) {
+		if (current->closid)
+			partid = current->closid;
+	}
+
+	if (static_branch_likely(&resctrl_mon_enable_key)) {
+		if (current->rmid)
+			pmg = current->rmid;
+	}
+
+	if (partid != state->cur_closid || pmg != state->cur_rmid) {
+		u64 reg;
+		state->cur_closid = partid;
+		state->cur_rmid = pmg;
+
+		/* set in EL0 */
+		reg = mpam_read_sysreg_s(SYS_MPAM0_EL1, "SYS_MPAM0_EL1");
+		reg = PARTID_SET(reg, partid);
+		reg = PMG_SET(reg, pmg);
+		mpam_write_sysreg_s(reg, SYS_MPAM0_EL1, "SYS_MPAM0_EL1");
+
+		/* set in EL1 */
+		reg = mpam_read_sysreg_s(SYS_MPAM1_EL1, "SYS_MPAM1_EL1");
+		reg = PARTID_SET(reg, partid);
+		reg = PMG_SET(reg, pmg);
+		mpam_write_sysreg_s(reg, SYS_MPAM1_EL1, "SYS_MPAM1_EL1");
+	}
+}
diff --git a/arch/arm64/kernel/mpam_ctrlmon.c b/arch/arm64/kernel/mpam_ctrlmon.c
new file mode 100644
index 000000000000..78bc26272340
--- /dev/null
+++ b/arch/arm64/kernel/mpam_ctrlmon.c
@@ -0,0 +1,527 @@
+/*
+ * Resource Director Technology(RDT)
+ * - Cache Allocation code.
+ *
+ * Copyright (C) 2016 Intel Corporation
+ *
+ * Authors:
+ *    Fenghua Yu <fenghua.yu@intel.com>
+ *    Tony Luck <tony.luck@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * More information about RDT be found in the Intel (R) x86 Architecture
+ * Software Developer Manual June 2016, volume 3, section 17.17.
+ */
+
+#define pr_fmt(fmt)	KBUILD_MODNAME ": " fmt
+
+#include <linux/kernfs.h>
+#include <linux/seq_file.h>
+#include <linux/slab.h>
+#include <linux/resctrlfs.h>
+
+#include <asm/mpam.h>
+#include <asm/mpam_resource.h>
+#include <asm/resctrl.h>
+
+/*
+ * Check whether a cache bit mask is valid. The SDM says:
+ *	Please note that all (and only) contiguous '1' combinations
+ *	are allowed (e.g. FFFFH, 0FF0H, 003CH, etc.).
+ * Additionally Haswell requires at least two bits set.
+ */
+static bool cbm_validate(char *buf, unsigned long *data, struct raw_resctrl_resource *r)
+{
+	u64 val;
+	int ret;
+
+	ret = kstrtou64(buf, 16, &val);
+	if (ret) {
+		rdt_last_cmd_printf("non-hex character in mask %s\n", buf);
+		return false;
+	}
+
+#if 0
+	if (val == 0 || val > r->default_ctrl) {
+		rdt_last_cmd_puts("mask out of range\n");
+		return false;
+	}
+#endif
+
+	*data = val;
+	return true;
+}
+
+/*
+ * Read one cache bit mask (hex). Check that it is valid for the current
+ * resource type.
+ */
+int parse_cbm(char *buf, struct raw_resctrl_resource *r, struct rdt_domain *d)
+{
+	unsigned long data;
+
+	if (d->have_new_ctrl) {
+		rdt_last_cmd_printf("duplicate domain %d\n", d->id);
+		return -EINVAL;
+	}
+
+	if (!cbm_validate(buf, &data, r))
+		return -EINVAL;
+
+	d->new_ctrl = data;
+	d->have_new_ctrl = true;
+
+	return 0;
+}
+
+/*
+ * For each domain in this resource we expect to find a series of:
+ * id=mask
+ * separated by ";". The "id" is in decimal, and must match one of
+ * the "id"s for this resource.
+ */
+static int parse_line(char *line, struct resctrl_resource *r)
+{
+	struct raw_resctrl_resource *rr = (struct raw_resctrl_resource *)r->res;
+	char *dom = NULL, *id;
+	struct rdt_domain *d;
+	unsigned long dom_id;
+
+
+next:
+	if (!line || line[0] == '\0')
+		return 0;
+	dom = strsep(&line, ";");
+	id = strsep(&dom, "=");
+	if (!dom || kstrtoul(id, 10, &dom_id)) {
+		rdt_last_cmd_puts("Missing '=' or non-numeric domain\n");
+		return -EINVAL;
+	}
+	dom = strim(dom);
+	list_for_each_entry(d, &r->domains, list) {
+		if (d->id == dom_id) {
+			if (rr->parse_ctrlval(dom, (struct raw_resctrl_resource *)&r->res, d))
+				return -EINVAL;
+			goto next;
+		}
+	}
+	return -EINVAL;
+}
+
+static int update_domains(struct resctrl_resource *r, int partid)
+{
+	struct raw_resctrl_resource *rr;
+	struct rdt_domain *d;
+
+	rr = (struct raw_resctrl_resource *)r->res;
+	list_for_each_entry(d, &r->domains, list) {
+		if (d->have_new_ctrl && d->new_ctrl != d->ctrl_val[partid]) {
+			d->ctrl_val[partid] = d->new_ctrl;
+			rr->msr_update(d, partid);
+		}
+	}
+
+	return 0;
+}
+
+static int resctrl_group_parse_resource(char *resname, char *tok, int closid)
+{
+	struct resctrl_resource *r;
+	struct raw_resctrl_resource *rr;
+
+	for_each_resctrl_resource(r) {
+		if (r->alloc_enabled) {
+			rr = (struct raw_resctrl_resource *)r->res;
+			if (!strcmp(resname, r->name) && closid < rr->num_partid)
+				return parse_line(tok, r);
+		}
+	}
+	rdt_last_cmd_printf("unknown/unsupported resource name '%s'\n", resname);
+	return -EINVAL;
+}
+
+ssize_t resctrl_group_schemata_write(struct kernfs_open_file *of,
+				char *buf, size_t nbytes, loff_t off)
+{
+	struct rdtgroup *rdtgrp;
+	struct rdt_domain *dom;
+	struct resctrl_resource *r;
+	char *tok, *resname;
+	int closid, ret = 0;
+
+	/* Valid input requires a trailing newline */
+	if (nbytes == 0 || buf[nbytes - 1] != '\n')
+		return -EINVAL;
+	buf[nbytes - 1] = '\0';
+
+	rdtgrp = resctrl_group_kn_lock_live(of->kn);
+	if (!rdtgrp) {
+		resctrl_group_kn_unlock(of->kn);
+		return -ENOENT;
+	}
+	rdt_last_cmd_clear();
+
+	closid = rdtgrp->closid;
+
+	for_each_resctrl_resource(r) {
+		if (r->alloc_enabled) {
+			list_for_each_entry(dom, &r->domains, list)
+				dom->have_new_ctrl = false;
+		}
+	}
+
+	while ((tok = strsep(&buf, "\n")) != NULL) {
+		resname = strim(strsep(&tok, ":"));
+		if (!tok) {
+			rdt_last_cmd_puts("Missing ':'\n");
+			ret = -EINVAL;
+			goto out;
+		}
+		if (tok[0] == '\0') {
+			rdt_last_cmd_printf("Missing '%s' value\n", resname);
+			ret = -EINVAL;
+			goto out;
+		}
+		ret = resctrl_group_parse_resource(resname, tok, closid);
+		if (ret)
+			goto out;
+	}
+
+	for_each_resctrl_resource(r) {
+		if (r->alloc_enabled) {
+			ret = update_domains(r, closid);
+			if (ret)
+				goto out;
+		}
+	}
+
+out:
+	resctrl_group_kn_unlock(of->kn);
+	return ret ?: nbytes;
+}
+
+static void show_doms(struct seq_file *s, struct resctrl_resource *r, int partid)
+{
+	struct raw_resctrl_resource *rr = (struct raw_resctrl_resource *)r->res;
+	struct rdt_domain *dom;
+	bool sep = false;
+
+	seq_printf(s, "%*s:", max_name_width, r->name);
+	list_for_each_entry(dom, &r->domains, list) {
+		if (sep)
+			seq_puts(s, ";");
+		seq_printf(s, rr->format_str, dom->id, max_data_width,
+			   rr->msr_read(dom, partid));
+		sep = true;
+	}
+	seq_puts(s, "\n");
+}
+
+int resctrl_group_schemata_show(struct kernfs_open_file *of,
+			   struct seq_file *s, void *v)
+{
+	struct rdtgroup *rdtgrp;
+	struct resctrl_resource *r;
+	struct raw_resctrl_resource *rr;
+	int ret = 0;
+	u32 partid;
+
+	rdtgrp = resctrl_group_kn_lock_live(of->kn);
+	if (rdtgrp) {
+		partid = rdtgrp->closid;
+		for_each_resctrl_resource(r) {
+			if (r->alloc_enabled) {
+				rr = (struct raw_resctrl_resource *)r->res;
+				if (partid < rr->num_partid)
+					show_doms(s, r, partid);
+			}
+		}
+	} else {
+		ret = -ENOENT;
+	}
+	resctrl_group_kn_unlock(of->kn);
+	return ret;
+}
+
+/*
+ * [FIXME]
+ * use pmg as monitor id
+ * just use match_pardid only.
+ */
+static u64 mbwu_read(struct rdt_domain *d, struct rdtgroup *g)
+{
+	u32 pmg = g->mon.rmid;
+
+	mpam_writel(pmg, d->base + MSMON_CFG_MON_SEL);
+	return mpam_readl(d->base + MSMON_MBWU);
+}
+
+static u64 csu_read(struct rdt_domain *d, struct rdtgroup *g)
+{
+	u32 pmg = g->mon.rmid;
+
+	mpam_writel(pmg, d->base + MSMON_CFG_MON_SEL);
+	return mpam_readl(d->base + MSMON_CSU);
+}
+
+int resctrl_group_mondata_show(struct seq_file *m, void *arg)
+{
+	struct kernfs_open_file *of = m->private;
+	struct rdtgroup *rdtgrp;
+	struct rdt_domain *d;
+	int ret = 0;
+
+	rdtgrp = resctrl_group_kn_lock_live(of->kn);
+
+	d = of->kn->priv;
+
+	/* for debug */
+	seq_printf(m, "group: partid: %d, pmg: %d",
+		   rdtgrp->closid, rdtgrp->mon.rmid);
+
+	/* show monitor data */
+
+	resctrl_group_kn_unlock(of->kn);
+	return ret;
+}
+
+static struct kernfs_ops kf_mondata_ops = {
+	.atomic_write_len	= PAGE_SIZE,
+	.seq_show		= resctrl_group_mondata_show,
+};
+
+/* set uid and gid of resctrl_group dirs and files to that of the creator */
+static int resctrl_group_kn_set_ugid(struct kernfs_node *kn)
+{
+	struct iattr iattr = { .ia_valid = ATTR_UID | ATTR_GID,
+				.ia_uid = current_fsuid(),
+				.ia_gid = current_fsgid(), };
+
+	if (uid_eq(iattr.ia_uid, GLOBAL_ROOT_UID) &&
+	    gid_eq(iattr.ia_gid, GLOBAL_ROOT_GID))
+		return 0;
+
+	return kernfs_setattr(kn, &iattr);
+}
+
+#if 0	/* used at remove cpu*/
+/*
+ * Remove all subdirectories of mon_data of ctrl_mon groups
+ * and monitor groups with given domain id.
+ */
+void rmdir_mondata_subdir_allrdtgrp(struct resctrl_resource *r, unsigned int dom_id)
+{
+	struct resctrl_group *prgrp, *crgrp;
+	char name[32];
+
+	if (!r->mon_enabled)
+		return;
+
+	list_for_each_entry(prgrp, &resctrl_all_groups, resctrl_group_list) {
+		sprintf(name, "mon_%s_%02d", r->name, dom_id);
+		kernfs_remove_by_name(prgrp->mon.mon_data_kn, name);
+
+		list_for_each_entry(crgrp, &prgrp->mon.crdtgrp_list, mon.crdtgrp_list)
+			kernfs_remove_by_name(crgrp->mon.mon_data_kn, name);
+	}
+}
+#endif
+
+static int mkdir_mondata_subdir(struct kernfs_node *parent_kn,
+				struct rdt_domain *d,
+				struct resctrl_resource *r, struct resctrl_group *prgrp)
+{
+#if 1
+	struct kernfs_node *kn;
+	char name[32];
+	int ret;
+
+	sprintf(name, "mon_%s_%02d", r->name, d->id);
+
+	kn = __kernfs_create_file(parent_kn, name, 0444,
+				  GLOBAL_ROOT_UID, GLOBAL_ROOT_GID, 0,
+				  &kf_mondata_ops, d, NULL, NULL);
+	if (IS_ERR(kn))
+		return PTR_ERR(kn);
+
+	ret = resctrl_group_kn_set_ugid(kn);
+	if (ret) {
+		kernfs_remove(kn);
+		return ret;
+	}
+
+	return ret;
+#if 0
+	/* create the directory */
+	kn = kernfs_create_dir(parent_kn, name, parent_kn->mode, prgrp);
+	if (IS_ERR(kn))
+		return PTR_ERR(kn);
+
+	/*
+	 * This extra ref will be put in kernfs_remove() and guarantees
+	 * that kn is always accessible.
+	 */
+	kernfs_get(kn);
+	ret = resctrl_group_kn_set_ugid(kn);
+	if (ret)
+		goto out_destroy;
+#endif
+
+
+#if 0
+	ret = mon_addfile(kn, mevt->name, d);
+	if (ret)
+		goto out_destroy;
+
+	kernfs_activate(kn);
+	return 0;
+
+out_destroy:
+	kernfs_remove(kn);
+	return ret;
+#endif
+#else
+	return 0;
+#endif
+}
+
+/*
+ * Add all subdirectories of mon_data for "ctrl_mon" groups
+ * and "monitor" groups with given domain id.
+ */
+void mkdir_mondata_subdir_allrdtgrp(struct resctrl_resource *r,
+				    struct rdt_domain *d)
+{
+	struct kernfs_node *parent_kn;
+	struct resctrl_group *prgrp, *crgrp;
+	struct list_head *head;
+
+	if (!r->mon_enabled)
+		return;
+
+	list_for_each_entry(prgrp, &resctrl_all_groups, resctrl_group_list) {
+		parent_kn = prgrp->mon.mon_data_kn;
+		mkdir_mondata_subdir(parent_kn, d, r, prgrp);
+
+		head = &prgrp->mon.crdtgrp_list;
+		list_for_each_entry(crgrp, head, mon.crdtgrp_list) {
+			parent_kn = crgrp->mon.mon_data_kn;
+			mkdir_mondata_subdir(parent_kn, d, r, crgrp);
+		}
+	}
+}
+
+static int mkdir_mondata_subdir_alldom(struct kernfs_node *parent_kn,
+				       struct resctrl_resource *r,
+				       struct resctrl_group *prgrp)
+{
+	struct rdt_domain *dom;
+	int ret;
+
+	list_for_each_entry(dom, &r->domains, list) {
+		ret = mkdir_mondata_subdir(parent_kn, dom, r, prgrp);
+		if (ret)
+			return ret;
+	}
+
+	return 0;
+}
+
+int
+mongroup_create_dir(struct kernfs_node *parent_kn, struct resctrl_group *prgrp,
+		    char *name, struct kernfs_node **dest_kn)
+{
+	struct kernfs_node *kn;
+	int ret;
+
+	/* create the directory */
+	kn = kernfs_create_dir(parent_kn, name, parent_kn->mode, prgrp);
+	if (IS_ERR(kn))
+		return PTR_ERR(kn);
+
+	if (dest_kn)
+		*dest_kn = kn;
+
+	/*
+	 * This extra ref will be put in kernfs_remove() and guarantees
+	 * that @rdtgrp->kn is always accessible.
+	 */
+	kernfs_get(kn);
+
+	ret = resctrl_group_kn_set_ugid(kn);
+	if (ret)
+		goto out_destroy;
+
+	kernfs_activate(kn);
+
+	return 0;
+
+out_destroy:
+	kernfs_remove(kn);
+	return ret;
+}
+
+
+/*
+ * This creates a directory mon_data which contains the monitored data.
+ *
+ * mon_data has one directory for each domain whic are named
+ * in the format mon_<domain_name>_<domain_id>. For ex: A mon_data
+ * with L3 domain looks as below:
+ * ./mon_data:
+ * mon_L3_00
+ * mon_L3_01
+ * mon_L3_02
+ * ...
+ *
+ * Each domain directory has one file per event:
+ * ./mon_L3_00/:
+ * llc_occupancy
+ *
+ */
+int mkdir_mondata_all(struct kernfs_node *parent_kn,
+			     struct resctrl_group *prgrp,
+			     struct kernfs_node **dest_kn)
+{
+	struct resctrl_resource *r;
+	struct kernfs_node *kn;
+	int ret;
+
+	/*
+	 * Create the mon_data directory first.
+	 */
+	ret = mongroup_create_dir(parent_kn, NULL, "mon_data", &kn);
+	if (ret)
+		return ret;
+
+	if (dest_kn)
+		*dest_kn = kn;
+
+	/*
+	 * Create the subdirectories for each domain. Note that all events
+	 * in a domain like L3 are grouped into a resource whose domain is L3
+	 */
+	for_each_resctrl_resource(r) {
+		if (r->mon_enabled) {
+			ret = mkdir_mondata_subdir_alldom(kn, r, prgrp);
+			if (ret)
+				goto out_destroy;
+		}
+	}
+
+	kernfs_activate(kn);
+
+	return 0;
+
+out_destroy:
+	kernfs_remove(kn);
+	return ret;
+}
diff --git a/arch/arm64/kernel/mpam_mon.c b/arch/arm64/kernel/mpam_mon.c
new file mode 100644
index 000000000000..ebf8980e072b
--- /dev/null
+++ b/arch/arm64/kernel/mpam_mon.c
@@ -0,0 +1,81 @@
+/*
+ * Resource Director Technology(RDT)
+ * - Monitoring code
+ *
+ * Copyright (C) 2017 Intel Corporation
+ *
+ * Author:
+ *    Vikas Shivappa <vikas.shivappa@intel.com>
+ *
+ * This replaces the cqm.c based on perf but we reuse a lot of
+ * code and datastructures originally from Peter Zijlstra and Matt Fleming.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * More information about RDT be found in the Intel (R) x86 Architecture
+ * Software Developer Manual June 2016, volume 3, section 17.17.
+ */
+
+#include <linux/module.h>
+#include <linux/slab.h>
+#include <linux/resctrlfs.h>
+
+#include <asm/resctrl.h>
+
+/*
+ * Global boolean for rdt_monitor which is true if any
+ * resource monitoring is enabled.
+ */
+bool rdt_mon_capable;
+
+static int pmg_free_map;
+
+void pmg_init(void)
+{
+	int pmg_max = 16;
+
+	pmg_free_map = BIT_MASK(pmg_max) - 1;
+
+	/* pmg 0 is always reserved for the default group */
+	pmg_free_map &= ~1;
+}
+
+int alloc_pmg(void)
+{
+	u32 pmg = ffs(pmg_free_map);
+
+	if (pmg == 0)
+		return -ENOSPC;
+	pmg--;
+	pmg_free_map &= ~(1 << pmg);
+
+	return pmg;
+}
+
+void free_pmg(u32 pmg)
+{
+	pmg_free_map |= 1 << pmg;
+}
+
+
+/*
+ * As of now the RMIDs allocation is global.
+ * However we keep track of which packages the RMIDs
+ * are used to optimize the limbo list management.
+ */
+int alloc_rmid(void)
+{
+	return alloc_pmg();
+}
+
+void free_rmid(u32 pmg)
+{
+	free_pmg(pmg);
+}
diff --git a/arch/arm64/kernel/mpam_resource.c b/arch/arm64/kernel/mpam_resource.c
new file mode 100644
index 000000000000..9cdda91ecb67
--- /dev/null
+++ b/arch/arm64/kernel/mpam_resource.c
@@ -0,0 +1,14 @@
+#include <linux/io.h>
+#include <linux/module.h>
+#include <linux/interrupt.h>
+#include <linux/platform_device.h>
+#include <linux/delay.h>
+#include <linux/of.h>
+#include <linux/of_irq.h>
+#include <linux/of_address.h>
+#include <linux/kthread.h>
+#include <linux/resctrlfs.h>
+
+#include <asm/mpam_resource.h>
+#include <asm/mpam.h>
+
diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 21e0f279982e..4f3310e95478 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -56,6 +56,7 @@
 #include <asm/processor.h>
 #include <asm/pointer_auth.h>
 #include <asm/stacktrace.h>
+#include <asm/mpam_sched.h>
 
 #if defined(CONFIG_STACKPROTECTOR) && !defined(CONFIG_STACKPROTECTOR_PER_TASK)
 #include <linux/stackprotector.h>
@@ -583,6 +584,8 @@ __notrace_funcgraph struct task_struct *__switch_to(struct task_struct *prev,
 	/* the actual thread switch */
 	last = cpu_switch_to(prev, next);
 
+	mpam_sched_in();
+
 	return last;
 }
 
diff --git a/fs/Kconfig b/fs/Kconfig
index aa4c12282301..ba8e9ad74f24 100644
--- a/fs/Kconfig
+++ b/fs/Kconfig
@@ -350,3 +350,12 @@ config IO_WQ
 	bool
 
 endmenu
+
+config RESCTRL
+	bool "Support Memory Partitioning and Monitoring"
+	default n
+	depends on INTEL_RDT || MPAM
+	select KERNFS
+	help
+	  Memory Partitioning and Monitoring. More exactly Memory system
+	  performance resource Partitioning and Monitoring
diff --git a/fs/resctrlfs.c b/fs/resctrlfs.c
index 5f3af5e0bae1..5b00f2bf22ff 100644
--- a/fs/resctrlfs.c
+++ b/fs/resctrlfs.c
@@ -73,6 +73,7 @@ static int resctrl_group_add_file(struct kernfs_node *parent_kn, struct rftype *
 	int ret;
 
 	kn = __kernfs_create_file(parent_kn, rft->name, rft->mode,
+				  GLOBAL_ROOT_UID, GLOBAL_ROOT_GID,
 				  0, rft->kf_ops, rft, NULL, NULL);
 	if (IS_ERR(kn))
 		return PTR_ERR(kn);
@@ -111,7 +112,7 @@ static int __resctrl_group_add_files(struct kernfs_node *kn, unsigned long fflag
 				     struct rftype *rfts, int len)
 {
 	struct rftype *rft;
-	int ret;
+	int ret = 0;
 
 	lockdep_assert_held(&resctrl_group_mutex);
 
@@ -138,7 +139,7 @@ static int __resctrl_group_add_files(struct kernfs_node *kn, unsigned long fflag
 
 static int resctrl_group_add_files(struct kernfs_node *kn, unsigned long fflags)
 {
-	int ret;
+	int ret = 0;
 
 	if (res_common_files)
 		ret = __resctrl_group_add_files(kn, fflags, res_common_files,
@@ -187,19 +188,23 @@ static int resctrl_group_create_info_dir(struct kernfs_node *parent_kn)
 	if (ret)
 		goto out_destroy;
 
-	for_each_alloc_enabled_resctrl_resource(r) {
-		fflags =  r->fflags | RF_CTRL_INFO;
-		ret = resctrl_group_mkdir_info_resdir(r, r->name, fflags);
-		if (ret)
-			goto out_destroy;
+	for_each_resctrl_resource(r) {
+		if (r->alloc_enabled) {
+			fflags =  r->fflags | RF_CTRL_INFO;
+			ret = resctrl_group_mkdir_info_resdir(r, r->name, fflags);
+			if (ret)
+				goto out_destroy;
+		}
 	}
 
-	for_each_mon_enabled_resctrl_resource(r) {
-		fflags =  r->fflags | RF_MON_INFO;
-		sprintf(name, "%s_MON", r->name);
-		ret = resctrl_group_mkdir_info_resdir(r, name, fflags);
-		if (ret)
-			goto out_destroy;
+	for_each_resctrl_resource(r) {
+		if (r->mon_enabled) {
+			fflags =  r->fflags | RF_MON_INFO;
+			sprintf(name, "%s_MON", r->name);
+			ret = resctrl_group_mkdir_info_resdir(r, name, fflags);
+			if (ret)
+				goto out_destroy;
+		}
 	}
 
 	/*
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2aa3ea5f7d41..f295f86dbc7a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1092,7 +1092,7 @@ struct task_struct {
 	/* cg_list protected by css_set_lock and tsk->alloc_lock: */
 	struct list_head		cg_list;
 #endif
-#ifdef CONFIG_X86_CPU_RESCTRL
+#if defined(CONFIG_RESCTRL) || defined(CONFIG_X86_CPU_RESCTRL)
 	u32				closid;
 	u32				rmid;
 #endif
-- 
2.31.1

