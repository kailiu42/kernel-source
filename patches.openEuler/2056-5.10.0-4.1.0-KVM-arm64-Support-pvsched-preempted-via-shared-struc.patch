Git-commit: 5606e8d3aa6b9cf7ef205868baa34dfb45e02315
From: Zengruan Ye <yezengruan@huawei.com>
Date: Tue, 2 Feb 2021 17:44:49 +0800
Subject: [PATCH] KVM: arm64: Support pvsched preempted via shared structure
Patch-mainline: Queued in openEuler repo, version 5.10.0-4.1.0
References: openEuler-21.03
Git-repo: https://gitee.com/openeuler/kernel.git

virt inclusion
category: feature
bugzilla: 47624
CVE: NA

--------------------------------

Implement the service call for configuring a shared structure between a
vCPU and the hypervisor in which the hypervisor can tell the vCPU that is
running or not.

Signed-off-by: Zengruan Ye <yezengruan@huawei.com>
Reviewed-by: Zhanghailiang <zhang.zhanghailiang@huawei.com>
Signed-off-by: Zheng Zengkai <zhengzengkai@huawei.com>
Signed-off-by: Kai Liu <kai.liu@suse.com>
---
 arch/arm64/include/asm/kvm_host.h | 16 ++++++++++++++++
 arch/arm64/kvm/arm.c              |  8 ++++++++
 arch/arm64/kvm/hypercalls.c       | 11 +++++++++++
 arch/arm64/kvm/pvsched.c          | 28 ++++++++++++++++++++++++++++
 4 files changed, 63 insertions(+)

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 872fb601d92a..4911931318e0 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -381,6 +381,11 @@ struct kvm_vcpu_arch {
 		u64 last_steal;
 		gpa_t base;
 	} steal;
+
+	/* Guest PV sched state */
+	struct {
+		gpa_t base;
+	} pvsched;
 };
 
 /* Pointer to the vcpu's SVE FFR for sve_{save,load}_state() */
@@ -590,6 +595,17 @@ static inline bool kvm_arm_is_pvtime_enabled(struct kvm_vcpu_arch *vcpu_arch)
 }
 
 long kvm_hypercall_pvsched_features(struct kvm_vcpu *vcpu);
+void kvm_update_pvsched_preempted(struct kvm_vcpu *vcpu, u32 preempted);
+
+static inline void kvm_arm_pvsched_vcpu_init(struct kvm_vcpu_arch *vcpu_arch)
+{
+	vcpu_arch->pvsched.base = GPA_INVALID;
+}
+
+static inline bool kvm_arm_is_pvsched_enabled(struct kvm_vcpu_arch *vcpu_arch)
+{
+	return (vcpu_arch->pvsched.base != GPA_INVALID);
+}
 
 void kvm_set_sei_esr(struct kvm_vcpu *vcpu, u64 syndrome);
 
diff --git a/arch/arm64/kvm/arm.c b/arch/arm64/kvm/arm.c
index bf77c2f51f65..73e6a7e5b072 100644
--- a/arch/arm64/kvm/arm.c
+++ b/arch/arm64/kvm/arm.c
@@ -288,6 +288,8 @@ int kvm_arch_vcpu_create(struct kvm_vcpu *vcpu)
 
 	kvm_arm_pvtime_vcpu_init(&vcpu->arch);
 
+	kvm_arm_pvsched_vcpu_init(&vcpu->arch);
+
 	vcpu->arch.hw_mmu = &vcpu->kvm->arch.mmu;
 
 	err = kvm_vgic_vcpu_init(vcpu);
@@ -393,6 +395,9 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 	update_steal_time(vcpu);
 	if (vcpu_has_ptrauth(vcpu))
 		vcpu_ptrauth_disable(vcpu);
+
+	if (kvm_arm_is_pvsched_enabled(&vcpu->arch))
+		kvm_update_pvsched_preempted(vcpu, 0);
 }
 
 void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
@@ -405,6 +410,9 @@ void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 	kvm_vcpu_pmu_restore_host(vcpu);
 
 	vcpu->cpu = -1;
+
+	if (kvm_arm_is_pvsched_enabled(&vcpu->arch))
+		kvm_update_pvsched_preempted(vcpu, 1);
 }
 
 static void vcpu_power_off(struct kvm_vcpu *vcpu)
diff --git a/arch/arm64/kvm/hypercalls.c b/arch/arm64/kvm/hypercalls.c
index 24624adf6bf8..a6fc39682a65 100644
--- a/arch/arm64/kvm/hypercalls.c
+++ b/arch/arm64/kvm/hypercalls.c
@@ -77,6 +77,17 @@ int kvm_hvc_call_handler(struct kvm_vcpu *vcpu)
 	case ARM_SMCCC_HV_PV_SCHED_FEATURES:
 		val = kvm_hypercall_pvsched_features(vcpu);
 		break;
+	case ARM_SMCCC_HV_PV_SCHED_IPA_INIT:
+		gpa = smccc_get_arg1(vcpu);
+		if (gpa != GPA_INVALID) {
+			vcpu->arch.pvsched.base = gpa;
+			val = SMCCC_RET_SUCCESS;
+		}
+		break;
+	case ARM_SMCCC_HV_PV_SCHED_IPA_RELEASE:
+		vcpu->arch.pvsched.base = GPA_INVALID;
+		val = SMCCC_RET_SUCCESS;
+		break;
 	default:
 		return kvm_psci_call(vcpu);
 	}
diff --git a/arch/arm64/kvm/pvsched.c b/arch/arm64/kvm/pvsched.c
index 3d96122fcf9e..b923c4b6c52e 100644
--- a/arch/arm64/kvm/pvsched.c
+++ b/arch/arm64/kvm/pvsched.c
@@ -5,9 +5,35 @@
  */
 
 #include <linux/arm-smccc.h>
+#include <linux/kvm_host.h>
+
+#include <asm/pvsched-abi.h>
 
 #include <kvm/arm_hypercalls.h>
 
+void kvm_update_pvsched_preempted(struct kvm_vcpu *vcpu, u32 preempted)
+{
+	struct kvm *kvm = vcpu->kvm;
+	u64 base = vcpu->arch.pvsched.base;
+	u64 offset = offsetof(struct pvsched_vcpu_state, preempted);
+	int idx;
+
+	if (base == GPA_INVALID)
+		return;
+
+	/*
+	 * This function is called from atomic context, so we need to
+	 * disable page faults.
+	 */
+	pagefault_disable();
+
+	idx = srcu_read_lock(&kvm->srcu);
+	kvm_put_guest(kvm, base + offset, cpu_to_le32(preempted));
+	srcu_read_unlock(&kvm->srcu, idx);
+
+	pagefault_enable();
+}
+
 long kvm_hypercall_pvsched_features(struct kvm_vcpu *vcpu)
 {
 	u32 feature = smccc_get_arg1(vcpu);
@@ -15,6 +41,8 @@ long kvm_hypercall_pvsched_features(struct kvm_vcpu *vcpu)
 
 	switch (feature) {
 	case ARM_SMCCC_HV_PV_SCHED_FEATURES:
+	case ARM_SMCCC_HV_PV_SCHED_IPA_INIT:
+	case ARM_SMCCC_HV_PV_SCHED_IPA_RELEASE:
 		val = SMCCC_RET_SUCCESS;
 		break;
 	}
-- 
2.31.1

